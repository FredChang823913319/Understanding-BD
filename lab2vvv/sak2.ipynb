{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Classification (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Artur Sak (sak2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due February 2, 2017 12:00 AM [ This is when Wednesday transitions to Thursday. ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistics and Lab Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [course website](https://courses.engr.illinois.edu/ece398bd/logisticsvvv.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What You Will Need To Know For This Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab covers some basic classifiers which can be used for M-ary classification.\n",
    "\n",
    "- k-Nearest Neighbors\n",
    "- Bayes Classifiers\n",
    "- Linear Discriminant Analysis\n",
    "\n",
    "There are some problems which have short answer questions. <b>Do not write an essay -- a few (1-2) complete sentences will suffice.</b>\n",
    "\n",
    "Be clear about your answers. For example, if a question asks you \"Which classifier would you choose?\", be unequivocal about which classifier you would choose (and why); as engineers, part of your job is to make design decisions and justify them in comparison to the alternatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preamble (Don't change this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as dist\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data needed for Problems 1-3 \n",
    "\n",
    "# Read the data\n",
    "traindata_tmp= genfromtxt('train.csv', delimiter=',')\n",
    "valdata_tmp= genfromtxt('val.csv', delimiter=',')\n",
    "\n",
    "#The data which you will use to train LDA and kNN is called \"trainingdata\"\n",
    "trainingdata=traindata_tmp[:,:2]\n",
    "#The corresponding labels are in \"traininglabels\"\n",
    "traininglabels=traindata_tmp[:,2]\n",
    "\n",
    "#The data which you will use to validate LDA, kNN and the Bayes Classifier\n",
    "#is called \"valdata\"\n",
    "valdata=valdata_tmp[:,:2]\n",
    "#The corresponding labels are in \"vallabels\"\n",
    "vallabels=valdata_tmp[:,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some code to visualize decision regions in Problem 1 to 3; You don't need to look at this.\n",
    "adp=np.vstack([trainingdata,valdata])\n",
    "xmin,xmax = adp[:,0].min()-1, adp[:,0].max()+1\n",
    "ymin,ymax = adp[:,0].min()-1, adp[:,0].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(xmin, xmax, 0.05),np.arange(ymin, ymax, 0.05))\n",
    "drdata= np.c_[xx.ravel(), yy.ravel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 : Bayes Classifiers (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement a Bayes classifier for the following $M$-ary classification problem:\n",
    "\n",
    "$$H_y: \\mathbf{X} \\sim \\mathcal{N}(\\mathbf{\\mu}_y,{\\sf C}) \\qquad y=0,\\ldots,M-1$$\n",
    "\n",
    "i.e. the data is a $d$-dimensional Gaussian with a common covariance matrix $\\sf C$ among all classes, but the means are different (and there is a prior among the classes). Remember, when the mean vectors, covariance matrix and prior probabilities are known, no classifier can do better than the Bayes classifier.\n",
    "\n",
    "You will write a function which takes in 4 parameters:\n",
    "* A set of data to classify (with rows as feature vectors) as a $(V,d)$ numpy.ndarray (data)\n",
    "* A M-length vector with the prior probabilities of each class as a numpy.ndarray (pi)\n",
    "* A matrix with rows giving the class means as a $(M,d)$ numpy.ndarray (means)\n",
    "* The common covariance matrix as a $(d,d)$ numpy.ndarray (cov)\n",
    "\n",
    "It will output a length $V$ numpy.ndarray of the outputs of the classifier (labels). You may not use scikit-learn or similar to implement this. Note that the class labels in this problem are $0,1,2$ (not $1,2,3$). Since Python uses zero-based indexing, this will allow you to avoid a few +1's in your code. \n",
    "\n",
    "Some hints\n",
    "* If you did lab 1, exercises 5 and 6, they will get you through the bulk of this problem.\n",
    "* A non-exhaustive list of useful functions: numpy.linalg.inv, numpy.sum, numpy.log, numpy.argmax.\n",
    "* You may use <a href=\"http://docs.scipy.org/doc/numpy-1.10.1/user/basics.broadcasting.html\">broadcasting</a> to help simplify your code. The basic form you may want to use is, if you have code which says A + B where A is (n,m) and B is (m,) then numpy will automatically translate this to adding B to each row of A. \n",
    "\n",
    "\n",
    "A function prototype is provided below <b>(10 points)</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bayesClassifier(data,pi,means,cov):\n",
    "    m_cov_prod =  np.dot(means, np.linalg.inv(cov))\n",
    "    return np.argmax (np.log(pi) + m_cov_prod * np.transpose(data) - 1/2 * m_cov_prod * means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will write a function which calculates the error of a classifier using the $0,1$-loss by comparing the true labels and the predicted labels. \n",
    "\n",
    "The function will take in two parameters:\n",
    "* A vector of length $N$ with the true labels as a numpy.ndarray (truelabels)\n",
    "* A vector of length $N$ with the estimated labels as a numpy.ndarray (estimatedlabels)\n",
    "\n",
    "The function will return the error (a scalar).\n",
    "\n",
    "A function prototype is provided below <b>(5 points)</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifierError(truelabels,estimatedlabels):\n",
    "    # Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load some sample data, in the format specified above. \n",
    "We have three classes, with\n",
    "$$\\pi_0=\\frac{1}{3}, \\pi_1=\\frac{1}{2}, \\pi_2=\\frac{1}{6}$$\n",
    "\n",
    "$$\\mathbf{\\mu}_0=\\begin{bmatrix} 5 \\\\ 5\\end{bmatrix},\\mathbf{\\mu}_1=\\begin{bmatrix} 5 \\\\ 0\\end{bmatrix}, \\mathbf{\\mu}_2=\\begin{bmatrix} -1\\\\0\\end{bmatrix} $$\n",
    "\n",
    "$$\\Sigma=\\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The data which you will use to test the classifier is called \"data\"\n",
    "data=np.copy(valdata)\n",
    "#The labels are in \"truelabels\"\n",
    "truelabels=np.copy(vallabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data by class. Each class will be in a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatter(data[:,0],data[:,1],c=truelabels)\n",
    "axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the classifier on the data in `data` with labels `truelabels`. Store the predicted labels in a variable called `estimatedlabels` and report the classifier's error. Run the classifier on the data in `drdata` and store the labels outputted by the classifier into a variable called `drB`. We will use `drB` to help visualize the decision regions. <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,2) (2,240) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-049c7d755eb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpis\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mbayesClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-1e1670402009>\u001b[0m in \u001b[0;36mbayesClassifier\u001b[1;34m(data, pi, means, cov)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbayesClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mm_cov_prod\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mm_cov_prod\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mm_cov_prod\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmeans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2) (2,240) "
     ]
    }
   ],
   "source": [
    "cov   = np.array([[3,1],[1,3]])\n",
    "pis   = np.array([1/3, 1/2, 1/6])\n",
    "means = np.array([[5,5], [5,0], [-1, 0]])\n",
    "print bayesClassifier(np.array(data),pis, means, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put Your Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets visualize the output of our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcolormesh(xx,yy,drB.reshape(xx.shape),alpha=0.1,antialiased=True)\n",
    "axis('tight')\n",
    "scatter(data[:,0],data[:,1],c=truelabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe linear boundaries between the decision regions and almost all the points are in the correct region for this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2 : Linear Discriminant Analysis (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will implement Linear Discriminant Analysis (LDA). You will use the Bayes classifier from Problem 1 to do this. You will be given:\n",
    "* Training data feature vectors as a $(N,d)$ numpy.ndarray (trainfeat), where each row is a feature vector.\n",
    "* Training data labels as a length $N$ numpy.ndarray (trainlabel)\n",
    "\n",
    "The first function you will write will return a tuple of the estimates of the prior probabilities (as a $M$ length numpy.ndarray), means (as a $(M,d)$ numpy.ndarray) and covariance matrix (as a $(d,d)$ numpy.ndarray) in the LDA model. You may assume that labels $0,\\ldots,$trainlabel.max() exist in order to avoid some error checking. \n",
    "\n",
    "A hint:\n",
    "* You can use logical operations+slicing to index an array. For example, if you want to get all training feature vectors whose  labels are `i`, you can use `trainfeat[trainlabel==i]`\n",
    "\n",
    "A function prototype is provided below: <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainLDA(trainfeat,trainlabel):\n",
    "    nlabels=int(trainlabel.max())+1 #Assuming all labels up to nlabels exist.\n",
    "    pi=np.zeros(nlabels) # store your prior in here\n",
    "    means=np.zeros((nlabels,trainfeat.shape[1])) # store the class means in here\n",
    "    cov=np.zeros((trainfeat.shape[1],trainfeat.shape[1])) # store the covariance matrix in here\n",
    "    # Put your code here\n",
    "\n",
    "    return (pi,means,cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training data is in a numpy array called `trainingdata`, with corresponding labels `traininglabels`. Our validation data is in a numpy array called `valdata`, with corresponding labels `vallabels`. The data format is the same as Problem 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatter(trainingdata[:,0],trainingdata[:,1],c=traininglabels)\n",
    "axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the bayesClassifier function you wrote in Problem 1 along with the trainLDA function from Problem 2 to implement the LDA classifier. Train the LDA classifier on the training data in `trainingdata`, and then run the LDA classifier on the training data and the validation data. Store the predicted training labels in `estimatedtraininglabels` and the predicted labels on the validation data in `estimatedvallabels`. Calculate the corresponding errors. Make sure to display the prior, means and covariance estimated in LDA. Also, run your LDA classifier on the data in `drdata` and store the resultant predicted labels in `drLDA` to help visualize the output of the classifier. <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is generated with the distribution used in Problem 1, so your $\\pi, \\mu, {\\sf C}$ should all be pretty close to the ones given in Problem 1. If they are not close, you've done something wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the training error (error of the classifier on the training data) and the validation error (error of the classifier on the validation data). <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put Your Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the performance of the classifier on the training and validation data. In this problem, both the training and validation data was generated from the distributions specified in Problem 1, so we show both the LDA classifier (which you learned from the data) and the Bayes classifier (which assumed you knew the true joint distribution of the data and the labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(8, 8)) # If this is looking a bit squished, you can change the 8 (width) and 8 (height)\n",
    "subplot(2,2,1)\n",
    "pcolormesh(xx,yy,drLDA.reshape(xx.shape),alpha=0.1,antialiased=True)\n",
    "axis('tight')\n",
    "scatter(trainingdata[:,0],trainingdata[:,1],c=traininglabels)\n",
    "title('Training Data (LDA)')\n",
    "subplot(2,2,2)\n",
    "pcolormesh(xx,yy,drB.reshape(xx.shape),alpha=0.1,antialiased=True)\n",
    "axis('tight')\n",
    "scatter(trainingdata[:,0],trainingdata[:,1],c=traininglabels)\n",
    "title('Training Data (Bayes)')\n",
    "subplot(2,2,3)\n",
    "pcolormesh(xx,yy,drLDA.reshape(xx.shape),alpha=0.1,antialiased=True)\n",
    "axis('tight')\n",
    "scatter(valdata[:,0],valdata[:,1],c=vallabels)\n",
    "title('Validation Data (LDA)')\n",
    "subplot(2,2,4)\n",
    "pcolormesh(xx,yy,drB.reshape(xx.shape),alpha=0.1,antialiased=True)\n",
    "axis('tight')\n",
    "scatter(valdata[:,0],valdata[:,1],c=vallabels)\n",
    "title('Validation Data (Bayes)')\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the points should be correctly classified in both the training and validation data. If they are not, you've done something wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we see that the LDA classifier gives boundaries which are quite similar to the Bayes classifier (assuming you've implemented both correctly). If you had a lot of training data from an arbitrary distribution, would you expect the LDA classifier to give similar boundaries to the Bayes classifier? Why or why not? <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3: k-Nearest Neighbors + Some Short Answer Questions (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the k-Nearest Neighbors algorithm.\n",
    "\n",
    "Your function will take:\n",
    "* Training data feature vectors as a $(N,d)$ numpy.ndarray (trainfeat), where each row is a feature vector\n",
    "* Training data labels as a length $N$ numpy.ndarray (trainlabel)\n",
    "* Test data feature vectors as a $(V,d)$ numpy.ndarray (testfeat), where each row is a feature vector\n",
    "* The value of k\n",
    "\n",
    "Use the Euclidean distance (scipy.spatial.distance.cdist) as your dissimilarity measure. \n",
    "\n",
    "Your function should return a length $V$ numpy.ndarray vector of the estimated labels. This should take around 4 lines of code. Do not use the kNN implementation in scikit-learn or similar.\n",
    "\n",
    "Some functions which may be useful (read the documentation):\n",
    "* The numpy.argpartition function can be used to find the $k$ smallest elements of an array (via slicing)\n",
    "* scipy.stats.mode can find the most common element in an array. \n",
    "\n",
    "<b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kNN(trainfeat,trainlabel,testfeat, k):\n",
    "    #Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your k-Nearest Neighbors classifier with the training data in `trainingdata` and validation data in `valdata` from Problem 2, for $k=1,3,4,5$. Compute the training and validation error rates on the data from Problem 2. <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which value of k would you choose for the k-NN classifier? Why? Run your k-NN classifier with the chosen value of k on  the data in `drdata` and store the result in a variable called `drK`.<b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code here\n",
    "k= # Put the value of k you would choose in the variable k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put Your Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us visualize the decision boundaries of your chosen value of $k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(8, 8)) # If this is looking a bit squished, you can change the 8 (width) and 8 (height)\n",
    "subplot(2,2,1)\n",
    "pcolormesh(xx,yy,drK.reshape(xx.shape),alpha=0.1,antialiased=True)\n",
    "axis('tight')\n",
    "scatter(trainingdata[:,0],trainingdata[:,1],c=traininglabels)\n",
    "title('Training Data (%i-NN)'%k)\n",
    "subplot(2,2,2)\n",
    "pcolormesh(xx,yy,drB.reshape(xx.shape),alpha=0.1,antialiased=True)\n",
    "axis('tight')\n",
    "scatter(trainingdata[:,0],trainingdata[:,1],c=traininglabels)\n",
    "title('Training Data (Bayes)')\n",
    "subplot(2,2,3)\n",
    "pcolormesh(xx,yy,drK.reshape(xx.shape),alpha=0.1,antialiased=True)\n",
    "axis('tight')\n",
    "scatter(valdata[:,0],valdata[:,1],c=vallabels)\n",
    "title('Validation Data (%i-NN)'%k)\n",
    "subplot(2,2,4)\n",
    "pcolormesh(xx,yy,drB.reshape(xx.shape),alpha=0.1,antialiased=True)\n",
    "axis('tight')\n",
    "scatter(valdata[:,0],valdata[:,1],c=vallabels)\n",
    "title('Validation Data (Bayes)')\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Short Answer Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation data used in problems 1-3 was all drawn from the distribution described in problem 1.  Compare and contrast the results you got from LDA and k-Nearest Neighbors as well as the Bayes classifier from problem 1. Your answer should consider both computational resources as well as error performance. <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put Your Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is special about the training error when you only have the standard nearest neighbor (1-NN) algorithm versus other values of k in k-NN? <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put Your Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you did not have knowledge of the true distribution of the data, out of the classifiers discussed in problems 1-3 (Bayes, LDA, kNN with the k you selected above), which classifier would you prefer in this problem? Why? <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put Your Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4: LDA and kNN using scikit-learn <b>(20 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, you will be using other people's libraries to implement learning algorithms. In this problem, you will become familiar with scikit-learn's implementation of LDA and kNN.\n",
    "\n",
    "First, we will load a data set of digits drawn from zip codes written on US mail. This data set was designed to help get good algorithms to sort mail by zip code automatically. It has been preprocessed a bit, with details given <a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.info.txt\">here</a>. Each feature vector consists of $16^2$ real values representing grayscale values of a 16 by 16 image of a digit. The training data has 7291 samples, while the validation data has 2007 samples. Note that this is not the same dataset built into scikit-learn -- it is much larger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading the Data\n",
    "\n",
    "#Read in the Training Data\n",
    "traindata_tmp= genfromtxt('zip.train', delimiter=' ') \n",
    "\n",
    "#The training labels are stored in \"trainlabels\", training features in \"traindata\". Rows are feature vectors.\n",
    "trainlabels=traindata_tmp[:,0]\n",
    "traindata=traindata_tmp[:,1:]\n",
    "\n",
    "#Read in the Validation Data\n",
    "valdata_tmp= genfromtxt('zip.val', delimiter=' ') \n",
    "\n",
    "#The validation labels are stored in \"vallabels\", validation features in \"valdata\". Rows are feature vectors.\n",
    "vallabels=valdata_tmp[:,0]\n",
    "valdata=valdata_tmp[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit-learn's sklearn.neighbors.KNeighborsClassifier to run a Nearest Neighbor classifier (1-NN) on the validation data with the provided training set. Note that KNeighborsClassifier defaults to 5-NN. \n",
    "\n",
    "Measure the time for fitting the model and classification (the %timeit feature or time() or similar will be useful). Try the different algorithms possible to fit the model (ball tree, kd-tree and brute force, and specify the fastest one in your code). Make sure to calculate the error on the validation set.  <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run LDA on the validation data set with scikit-learn's sklearn.lda.LDA class. Measure the training time as well as the time used to classify the validation set. Make sure to calculate the error on the validation set.  <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.lda import LDA\n",
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the performance on the validation set, which algorithm would you pick? Your answer should also take into account computational resources required, error on the validation set, and the cost associated with making an error (in real life -- recall the source of the data). <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the 0,1-loss is appropriate error measure in this case? Why or why not? How can you use domain-specific knowledge to help improve performance for this application?\n",
    "\n",
    "If you are interested in this in more detail on this problem, see O. Matan et al., \"Reading Handwritten Digits: A ZIP Code Recognition System\", IEEE Computer, Vol 25, Number 7, pp 59-63, 1992 (<a href=\"http://yann.lecun.com/exdb/publis/pdf/matan-92.pdf\">tech report version here</a>). You do not need to look at this to answer the question. <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
