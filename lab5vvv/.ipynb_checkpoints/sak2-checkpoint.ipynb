{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 - Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: (netid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due: February 23, 2017 at 12:00 AM (This is when Wednesday transitions to Thursday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the <a href=\"https://courses.engr.illinois.edu/ece398bd/\">course website</a>. This is the last lab for this section of the course. Make sure to be up to date for the policies of the second part of the course. **You will have another lab next week and a different TA (who is not familiar with this lab), so it is in your best interests to finish this lab before next week's lab session.**\n",
    "\n",
    "There will be office hours on Monday, as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Will Need To Know For This Lab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eigendecomposition\n",
    "- Singular Value Decomposition\n",
    "- Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble (Don't change this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.spatial.distance as dist\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Interactive Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enable_interactive=False # If you want to rotate plots, set this to True. \n",
    "# When submitting your notebook, enable_interactive=False and run the whole notebook. \n",
    "# The interactive stuff can be a bit glitchy, so if you're having trouble, turn them off. \n",
    "if enable_interactive:\n",
    "    # These packages allow us to rotate plots and what not.\n",
    "    from IPython.display import display\n",
    "    from IPython.html.widgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Visualizing Principal Components (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will be implementing PCA, visualizing the principal components and using it to perform dimensionality reduction. \n",
    "\n",
    "Do not use a pre-written implementation of PCA for this problem (e.g. sklearn.decomposition.PCA). You should assume that the input data has been appropriately pre-processed to have zero-mean features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will generate some data.\n",
    "numpy.random.seed(seed=2232017)\n",
    "true_cov = np.array([[1,.2,.3],[.2,1,.6],[.3,.6,1]]) #This is the true covariance matrix \n",
    "                                                     #of the data. Do not use it in your code!\n",
    "data=(np.random.randn(1000,3)).dot(np.linalg.cholesky(true_cov).T) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we visualize the data using a 3D scatterplot. \n",
    "\n",
    "Our data is stored in a variable called `data` where each row is a feature vector (with three features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(data[:,0],data[:,1],data[:,2])\n",
    "if enable_interactive:\n",
    "    @interact(elev=(-90, 90), azim=(0, 360))\n",
    "    def view(elev, azim):\n",
    "        ax.view_init(elev, azim)\n",
    "        display(ax.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which implements PCA via the eigendecomposition. <b>(10 points)</b>\n",
    "\n",
    "You will be given as input:\n",
    "- A $(N,d)$ numpy array of data (with each row as a feature vector)\n",
    "\n",
    "Your function should return a tuple consisting of the PCA transformation matrix (which is $(d,d)$), and a vector consisting of the amount of variance explained in the data by each PCA feature. Note that the PCA features are ordered in decreasing amount of variance explained, by convention.\n",
    "\n",
    "Hints:\n",
    "- The function <a href=\"http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linalg.eigh.html\">numpy.linalg.eigh</a> will be useful. Note that it returns its eigenvalues in *ascending* order. `numpy.fliplr` or similar may be useful as well.\n",
    "- You can calculate the covariance matrix of the data by multiplying the data matrix with its transpose in the appropriate order, and scaling it. \n",
    "- Do not use numpy.cov -- we are assuming the data has zero mean beforehand, so the number of degrees of freedom is different (since the covariance estimate knows the mean in our case). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pcaeig(data):\n",
    "    #Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run PCA on your data. Store your PCA transformation in a variable called `W`, and the amount of variance explained by each PCA feature in a variable called `s`. Print out the principal components (i.e. the rows of `W`) along with the corresponding amount of variance explained. <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the principal components on top of our data. The first principal component is in red, and captures the most variance. The second principal component is in green, while the last principal component is in yellow.\n",
    "\n",
    "We generated our data from am *elliptical distribution*, so it should be easy to visualize these components as the axes of the data (which looks like an ellipsoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figb = plt.figure()\n",
    "axb = Axes3D(figb)\n",
    "axb.scatter(data[:,0],data[:,1],data[:,2],alpha=0.1)\n",
    "c=['r-','g-','y-']\n",
    "for var, pc,color in zip(s, W,c):\n",
    "    axb.plot([0, 2*var*pc[0]], [0, 2*var*pc[1]], [0, 2*var*pc[2]], color, lw=2)\n",
    "if enable_interactive:\n",
    "    @interact(elev=(-90, 90), azim=(0, 360))\n",
    "    def view(elev, azim):\n",
    "        axb.view_init(elev, azim)\n",
    "        display(axb.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, the red line should be longer than the green line which should be longer than the yellow line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, you will implement functions to generate PCA features.\n",
    "\n",
    "Write a function which implements dimension reduction via PCA. It takes in three inputs:\n",
    "- A $(N,d)$ numpy array, `data`, with each row as a feature vector\n",
    "- A $(d,d)$ numpy array, `W`, the PCA transformation matrix (e.g. generated from `pcaeig` or `pcasvd`)\n",
    "- A number `k`, which is the number of PCA features to retain\n",
    "\n",
    "It should return a $(N,k)$ numpy array, where the $i$-th row contains the PCA features corresponding to the $i$-th input feature vector. <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pcadimreduce(data,W,k):\n",
    "    # Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which reconstructs the original features from the PCA features. It takes in three inputs:\n",
    "- A $(N,k)$ numpy array, `pcadata`, with each row as a PCA feature vector (e.g. generated from `pcadimreduce`)\n",
    "- A $(d,d)$ numpy array, `W`, the PCA transformation matrix (e.g. generated from `pcaeig` or `pcasvd`)\n",
    "- A number `k`, which is the number of PCA features\n",
    "\n",
    "It should return a $(N,d)$ numpy array, where the $i$-th row contains the reconstruction of the original $i$-th input feature vector (in `data`) based on the PCA features contained in `pcadata`. <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pcareconstruct(pcadata,W,k):\n",
    "    # Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, if you take $k=3$, perform dimensionality reduction then reconstruction, you should get the original data back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reconstructed data using all the principal components\n",
    "reduced_data=pcadimreduce(data,W,3)\n",
    "reconstructed_data=pcareconstruct(reduced_data,W,3)\n",
    "\n",
    "print \"This should be small:\",np.max(np.abs(data-reconstructed_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One use of PCA is to help visualize data. The 3-D plots above are a bit hard to read on a 2-D computer screen or when printed out. \n",
    "\n",
    "Use PCA to to reduce the data to two dimensions. Visualize the first two PCA features with a scatter plot. Also, construct an approximation of the original features using the first two principal components into a $(N,d)$ array called `reconstructed_data`.<b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the data using two principal components in the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figc = plt.figure()\n",
    "axc = Axes3D(figc)\n",
    "axc.scatter(reconstructed_data[:,0],reconstructed_data[:,1],reconstructed_data[:,2],alpha=0.1)\n",
    "c=['r-','g-','y-']\n",
    "for var, pc,color in zip(s, W,c):\n",
    "    axc.plot([0, 2*var*pc[0]], [0, 2*var*pc[1]], [0, 2*var*pc[2]], color, lw=2)\n",
    "    \n",
    "if enable_interactive:\n",
    "    @interact(elev=(-90, 90), azim=(0, 360))\n",
    "    def view(elev, azim):\n",
    "        axc.view_init(elev, azim)\n",
    "        display(axc.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, you should see no component of the data along the third principal direction, and the data should lie in a plane. This may be easier to see with the Interactive Mode on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PCA to reduce the data to one dimension and store the one dimensional PCA feature in `reduced_data_1`. Construct an approximation of the original features using the first  principal component into a $(N,d)$ array called `reconstructed_data_1`. <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize this in the original feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figd = plt.figure()\n",
    "axd = Axes3D(figd)\n",
    "axd.scatter(reconstructed_data_1[:,0],reconstructed_data_1[:,1],reconstructed_data_1[:,2],alpha=0.1)\n",
    "c=['r-','g-','y-']\n",
    "for var, pc,color in zip(s, W,c):\n",
    "    axd.plot([0, 2*var*pc[0]], [0, 2*var*pc[1]], [0, 2*var*pc[2]], color, lw=2)\n",
    "    \n",
    "if enable_interactive:\n",
    "    @interact(elev=(-90, 90), azim=(0, 360))\n",
    "    def view(elev, azim):\n",
    "        axd.view_init(elev, azim)\n",
    "        display(axd.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, you should see no component of the data along the second and third principal direction, and the data should lie along a line. This may be easier with the Interactive Mode on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the PCA feature as a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, bins, patches = hist(reduced_data_1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you will implement PCA via the SVD. <b>(5 points)</b>\n",
    "\n",
    "You will be given as input:\n",
    "- A $(N,d)$ numpy array of data (with each row as a feature vector)\n",
    "\n",
    "Your function should return a tuple consisting of the PCA transformation matrix, and a vector consisting of the amount of variance explained in the data by each PCA feature. Note that the PCA features are ordered in decreasing amount of variance explained.\n",
    "\n",
    "Hints:\n",
    "- The function <a href=\"http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linalg.svd.html\">numpy.linalg.svd</a> will be useful. Use the full SVD (default).\n",
    "- Be careful with how the SVD is returned in `numpy.linalg.svd` (`V` in numpy is the transpose of what is in the notes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pcasvd(data):\n",
    "    #Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your PCA implementation via the SVD is correct (and your Eigendecomposition implementation is correct), principal components should match between the SVD and PCA implementations (up to sign, i.e. the i-th principal component may be the negative of the i-th principal component from the eigendecomposition approach). \n",
    "\n",
    "Verify this by printing out the principal components and the corresponding amount of variance explained. **You will not get any credit if the principal components (up to sign) and variances do not match the eigendecomposition.** <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 2: PCA for Data Compression (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, you saw an example application of PCA to create eigenfaces. In this part of the lab, we will look at eigenfaces for compression using the <a href=\"http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\">Olivetti faces dataset</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we load the Olivetti dataset\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "\n",
    "oli = fetch_olivetti_faces()\n",
    "# Height and Width of Images are in h,w. You will need to reshape them to this size display them.\n",
    "h=64\n",
    "w=64\n",
    "X = oli.data\n",
    "\n",
    "X_t=X[-1]\n",
    "X=X[:-1]\n",
    "\n",
    "#This centering is unnecessary. it just makes the pictures a bit more readable. \n",
    "\n",
    "X_m=np.mean(X,axis=0)\n",
    "X=X-X_m # center them\n",
    "X_t=X_t-X_m\n",
    "\n",
    "# The data set is in X. You will compress the image X_t. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the Olivetti Faces:\n",
    "<img src=\"olivettifaces.gif\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be making use of Scikit-Learn's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">PCA</a> functionality. \n",
    "\n",
    "Three functions will be useful for this problem :\n",
    "- PCA.fit : Finds the requested number of principal components.\n",
    "- PCA.transform : Apply dimensionality reduction (returns the PCA features)\n",
    "- PCA.inverse_transform : Go from PCA features to the original features (Useful for visualizing)\n",
    "\n",
    "You will also find the following useful:\n",
    "- PCA.explained\\_variance\\_ratio\\_ : Percentage of variance explained by each of the principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fraction of **unexplained** variance on `X` by PCA retaining the first $k$ principal components, where  $k=1,\\ldots,200$. Note that this is a scree plot (normalized by the total variance). \n",
    "\n",
    "`numpy.cumsum` may be useful for this. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Scree plot, propose a reasonable number of principal components to keep, in order to perform dimensionality reduction. Justify your choice. <b>(5 points)</b>\n",
    "\n",
    "There is a range of correct answers (but you need to justify yours!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the first 5 principal components as well as the 30th, 50th and 100th principal components, which are called *eigenfaces* in this context. Assuming your PCA object is called `pca`, the eigenfaces are contained in `pca.components_`, where each row is a principal component. \n",
    "\n",
    "The following code from Lab 4 may be useful:\n",
    "\n",
    "    figure()\n",
    "    imshow( image , cmap = cm.Greys_r)\n",
    "    \n",
    "where image is the appropriately reshaped principal component (to `h` rows and `w` columns). What can you say about later eigenfaces compared to earlier ones? <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, you will compress an image, `X_t`, using PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is what X_t looks like:\n",
    "imshow((X_t).reshape((h,w)),cmap=cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the image in `X_t`'s approximation using the first `i` principal components (learned from `X`)  where i=1,10,20,...,100 (i.e. in increments of 10), then 120,140,160,180,200 (i.e. in increments of 20).\n",
    "\n",
    "Do this by the following procedure:\n",
    "1. Determine the PCA transformation (i.e. fit) on `X`. \n",
    "2. Transform `X_t` to the PCA features determined by `X`.\n",
    "3. Retain the first `i` PCA features of the transformed `X_t` (set the others to zero). \n",
    "4. Transform the result of step 3 back to the original feature space. \n",
    "\n",
    "<b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many principal components would you keep to compress the image? Why? (You may be qualitative or quantitative) <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: PCA for Classification (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load a data set of digits drawn from zip codes written on US mail. This data set was designed to help get good algorithms to sort mail by zip code automatically. It has been preprocessed a bit, with details given <a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.info.txt\">here</a>. Each feature vector consists of real values representing grayscale values of a 16 by 16 image of a digit. The training data has 7291 samples, while the validation data has 2007 samples. Note that this is not the same dataset built into scikit- learn -- it is much larger. Use sklearn.decomposition.PCA for this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the Data\n",
    "#Read in the Training Data\n",
    "traindata_tmp= genfromtxt('zip.train', delimiter=' ')\n",
    "#The training labels are stored in \"trainlabels\", training features in \"traindata\"\n",
    "trainlabels=traindata_tmp[:,0]\n",
    "traindata=traindata_tmp[:,1:]\n",
    "#Read in the Validation Data\n",
    "valdata_tmp= genfromtxt('zip.val', delimiter=' ')\n",
    "#The validation labels are stored in \"vallabels\", validation features in \"valdata\"\n",
    "vallabels=valdata_tmp[:,0]\n",
    "valdata=valdata_tmp[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab 2, you found that the validation error for 1-NN on this data set was 0.056. \n",
    "\n",
    "Make a plot of the validation error using 1-NN on the PCA features using 1,2,...,256 PCA features. What is the minimum validation error, and how many PCA features did you use? <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does PCA+NN give better or worse performance than just NN? Why do you think this is? Which would you choose (and if using PCA, with how many features)? Take into account both computational costs and error in your answer. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Spectral Clustering (Optional; Not Graded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By request, I've put an optional problem on spectral clustering. We won't look at it if you do it or not, but doing it might increase your proficiency with numpy arrays and what not. \n",
    "\n",
    "In this problem, you will implement a powerful clustering algorithm known as spectral clustering. It can separate data that in some cases, K-means cannot (as you will see in this problem).\n",
    "\n",
    "Spectral clustering works by forming a graph based on similarities between data vectors, and looking for cluster of data vectors such that the similarity between them is high, but the similarity to vectors outisde the clusters is low (and the clusters aren't too small).\n",
    "\n",
    "See Section 4.3 in the notes for details on how it works, or [this tutorial](https://arxiv.org/abs/0711.0189). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Spectral Clustering Algorithm (Alg. 9):**\n",
    "\n",
    "\n",
    "1. Let $\\tilde{L} = I - D^{-1/2} S D^{-1/2}$ where $D^{-1/2}$ is a square diagonal matrix with $\\frac{1}{\\sqrt{d_i}}$ as the $i$-th entry on the diagonal (where $\\mathbf{d} = S \\mathbf{1}$). \n",
    "2. Take the eigen-decomposition of $\\tilde{L}= U \\Lambda U^\\top$ where $\\Lambda$ is a diagonal matrix containing the eigenvalues of $L$.\n",
    "3. Let $U_K$ be a matrix whose columns are the eigenvectors corresponding to the $K$-smallest eigenvalues of $L$.\n",
    "4. Normalize each row of $U_K$ (i.e. divide each entry on the $i$-th row by the norm of the $i$-th row)\n",
    "5. Apply K-means clustering to the rows of $U_K$ (i.e. treat each row of $U_K$ as a $K$-dimensional feature vector and cluster it). \n",
    "6. Return the cluster labels from step $4$. $\\mathbf{x}_i$ is assigned to the cluster which the $i$-th row of $U_K$ was assigned to. \n",
    "\n",
    "$L$ is known as the normalized Laplacian of the similarity graph, and has many nice properties for analyzing the similarity graph, most of which are beyond the scope of the course. \n",
    "\n",
    "**Note: You don't really need to know why spectral clustering works to do this problem (though it would be nice) -- you just need to be able to implement the algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll make a data set based on the Illinois logo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp=np.nonzero(np.asarray(\n",
    "[[1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "[1,0,0,0,0,0,0,0,0,0,0,1],\n",
    "[1,0,0,0,0,0,0,0,0,0,0,1],\n",
    "[1,0,0,1,1,1,1,1,1,0,0,1],\n",
    "[1,0,0,0,0,1,1,0,0,0,0,1],\n",
    "[1,0,0,0,0,1,1,0,0,0,0,1],\n",
    "[1,0,0,0,0,1,1,0,0,0,0,1],\n",
    "[1,0,0,1,1,1,1,1,1,0,0,1],\n",
    "[1,0,0,0,0,0,0,0,0,0,0,1],\n",
    "[1,0,0,0,0,0,0,0,0,0,0,1],\n",
    "[1,1,1,1,1,1,1,1,1,1,1,1]]))\n",
    "\n",
    "illcmap=ListedColormap(['#131F33','#FA6300'])\n",
    "\n",
    "data=np.c_[tmp[1],tmp[0]]\n",
    "figure()\n",
    "scatter(data[:,0],data[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first see what happens if we try to cluster these points using K-means to get 2 clusters. \n",
    "\n",
    "Use `sklearn.cluster.KMeans` to cluster these points into two clusters. \n",
    "\n",
    "Plot the clusters using the colors as the labels you get from K-means clustering as a scatter plot, with `cmap=illcmap`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, you should see something like the right half of the points are in one cluster, and the left half are in the other. The Illinois I should not be separated from the perimeter. In general, K-means cannot produce non-convex clusters (i.e. if you draw a line between any 2 points in a cluster, any point that lies on that line is in that cluster), so it cannot separate the I from the border."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement spectral clustering as described above. \n",
    "\n",
    "Recall that `numpy.linalg.eigh` returns the eigenvalues of a matrix in *ascending* order.\n",
    "\n",
    "The code provided already calculates $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spectralClustering(data,K,C=1):\n",
    "    W=np.exp(-dist.cdist(data,data,'sqeuclidean')/C)\n",
    "    W=W-np.diag(np.diag(W))\n",
    "    Dinv5=np.diag( (W.dot(np.ones(W.shape[0])))**(-0.5) )\n",
    "    L=np.eye(W.shape[0])-Dinv5.dot(W).dot(Dinv5)\n",
    "    # Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run your spectral clustering implementation with 2 clusters on the data in `data`, and plot the data with the colors given by the clusters returned by `spectralClustering` with `cmap=illcmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default value of $C=1$ in the spectral clustering code should separate the I from the border (though which will be colored orange and which will be blue will be random)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you're doing audio and visual analytics next, I thought I'd leave you with something on that note.\n",
    "\n",
    "You can find a demo of Spectral Clustering applied to image segmentation based on \n",
    "\n",
    "Shi, Jianbo, and Jitendra Malik. \"Normalized cuts and image segmentation.\" IEEE Transactions on pattern analysis and machine intelligence 22.8 (2000): 888-905.\n",
    "\n",
    "at http://scikit-learn.org/stable/auto_examples/cluster/plot_face_segmentation.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# And this concludes the Machine Learning section of the course! Good luck with your future endeavors!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
