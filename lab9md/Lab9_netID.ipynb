{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:\n",
    "\n",
    "NetID:\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this lab, we will investigate two highly useful features for image analysis: color histogram and HOG. Both of these features are widely used in computer vision and image analysis problems of all kinds, such as face recognition, similar image search, image classification, and object detection.\n",
    "\n",
    "If you are using SageMathCloud, opencv and tensorflow are already installed, you can skip this note.\n",
    "\n",
    "## Notes on OpenCV\n",
    "\n",
    "Depending on the OS you are using there might be different instructions on how to build and install OpenCV with Python. We will be using OpenCV 3.1.0 for this lab. Older versions should work as well, but you might have to change some lines of code. Depending on your comfort level, you may choose either pathway.\n",
    "\n",
    "Make sure to at least get OpenCV working before you leave the lab today. Search online for how to install OpenCV on your OS. Installing from prebuilt binaries are recommended. If you installed successfully, you can run \n",
    "\n",
    "\\>>> import cv2\n",
    "\n",
    "\\>>> print cv2.\\__version\\__\n",
    "\n",
    "**DO NOT SUBMIT ANY OpenCV files.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Color Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For image analysis, much of the information of an image can be conveyed merely by its histogram, although other, more sofisticated features, such as SIFT, are often used and needed in big data problems. Here, we'll investigate how useful the histogram can be. For this section, you will need to download the Caltech-101 dataset (http://www.vision.caltech.edu/Image_Datasets/Caltech101/). This dataset contains 101 different object categories, each with between 40 to 800 images of size 300 x 200, roughly. (This dataset pales in comparison to other modern, \"big data\" image datasets, but it's the best that your computer can likely handle easily.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll look at three test-case images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beach = misc.imread('beach.png')\n",
    "desert1 = misc.imread('desert1.jpg')\n",
    "desert2 = misc.imread('desert2.jpg')\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(beach)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(desert1)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(desert2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the following two functions to generate and compare histograms. They are built upon OpenCV's implementation [compareHist](http://docs.opencv.org/2.4/modules/imgproc/doc/histograms.html). We use the CV_COMP_INTERSECT method. \n",
    "The image has three channels, the comparison results are stored in x1, x2 and x3. And Euclidean distance is used to give a single output number **comp**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This function is used to compare two histograms using the histogram intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compHist(h1, h2):\n",
    "    # h1, h2 - input histograms to compare, which should be of dimension (3,N) for color images and (1,N) for\n",
    "    #              grayscale images. If the two histograms are not the same dimensions, the output is a large\n",
    "    #              value to enforce their disimilarity\n",
    "    if h1.shape[0] != h2.shape[0]:\n",
    "        return 100\n",
    "    else:\n",
    "        x1 = cv2.compareHist(h1[0,:].astype('float32'), h2[0,:].astype('float32'), 3)\n",
    "        x2 = cv2.compareHist(h1[1,:].astype('float32'), h2[1,:].astype('float32'), 3)\n",
    "        x3 = cv2.compareHist(h1[2,:].astype('float32'), h2[2,:].astype('float32'), 3)\n",
    "        comp = np.sqrt(x1**2 + x2**2 + x3**2)\n",
    "        return comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This function is used to generates a histogram from an input image.  Notice how we normalize the histogram by dividing by the total pixels in the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generateHist(img, n_bins):\n",
    "    # img - the input image\n",
    "    # n_bins - the number of bins of the output histogram\n",
    "    # returns: h - the histogram\n",
    "    k = len(img.shape)\n",
    "    if k == 2:\n",
    "        h = np.zeros((1,n_bins),dtype='float32')\n",
    "        M,N = img.shape\n",
    "        h[0,:][:, np.newaxis] = cv2.calcHist([img], [0], None, [n_bins], [0,256])\n",
    "        h = h/(M*N)\n",
    "    else:\n",
    "        h = np.zeros((3,n_bins),dtype='float32')\n",
    "        M,N,D = img.shape\n",
    "        h[0,:][:, np.newaxis] = cv2.calcHist([img], [0], None, [n_bins], [0,256])\n",
    "        h[1,:][:, np.newaxis] = cv2.calcHist([img], [1], None, [n_bins], [0,256])\n",
    "        h[2,:][:, np.newaxis] = cv2.calcHist([img], [2], None, [n_bins], [0,256])\n",
    "        h = h/(M*N)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the above functions to generate histograms for the three test-cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_bins = 100\n",
    "\n",
    "hist_desert1 = generateHist(desert1, n_bins)\n",
    "hist_desert2 = generateHist(desert2, n_bins)\n",
    "hist_beach = generateHist(beach, n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)\n",
    "f.set_size_inches((8,8))\n",
    "ax1.plot(hist_beach[0,:])\n",
    "ax1.plot(hist_beach[1,:])\n",
    "ax1.plot(hist_beach[2,:])\n",
    "ax1.set_title('Histograms of Beach and Desert Scenes')\n",
    "\n",
    "ax2.plot(hist_desert1[0,:])\n",
    "ax2.plot(hist_desert1[1,:])\n",
    "ax2.plot(hist_desert1[2,:])\n",
    "\n",
    "ax3.plot(hist_desert2[0,:])\n",
    "ax3.plot(hist_desert2[1,:])\n",
    "ax3.plot(hist_desert2[2,:])\n",
    "ax3.set_xlabel('Color Intensity Bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the method=CV_COMP_INTERSECT interesction as a comparison metric, we compare how \"similar\" are the histograms. Which two images are the most similar? (Remember that this metric is a distance, so smaller values mean more \"similar\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Beach vs. Desert1:\\t' + str(compHist(hist_beach, hist_desert1)) # beach vs. desert1\n",
    "print 'Desert2 vs. Desert1:\\t' + str(compHist(hist_desert2, hist_desert1)) # desert2 vs. desert1\n",
    "print 'Beach vs. Desert2:\\t' + str(compHist(hist_beach, hist_desert2)) # beach vs. desert2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Write your own implementation of the histogram. It should take in an image and a parameter called \"n_bins,\" which is the number of bins for the histogram. (In an image, pixel values range from 0 - 255, but the histogram could have fewer than 255 bins. If, for instance, n_bins = 100, then each bin of the histogram would store between 2-3 different pixel intensity values.) \n",
    "\n",
    "For grayscale images, the output will be a 1D vector of dimension (1, n_bins). For color images, compute a 1D histogram of each channel with n_bins and form the three histograms into a 2D matrix of dimension (3, n_bins). The histogram should also be normalized by the total number of pixels, so that along each row it sums to 1. \n",
    "\n",
    "**You are NOT allowed to use any OpenCV or third party libraries.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def MygenerateHist(image, n_bins):\n",
    "    #Your code here\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Write your own implementation of the histogram comparing. See the formula in the Intersection (method=CV_COMP_INTERSECT) OpenCV for how to implement the histogram intersection. \n",
    "\n",
    "**You are NOT allowed to use any OpenCV or third party libraries.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MycompHist(h1, h2):\n",
    "        #Your code here\n",
    "        return comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Testing your functions using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist_desert1 = MygenerateHist(desert1, nbins)\n",
    "hist_desert2 = MygenerateHist(desert2, nbins)\n",
    "hist_beach = MygenerateHist(beach, nbins)\n",
    "\n",
    "print 'Beach vs. Desert1:\\t' + str(MycompHist(hist_beach, hist_desert1)) # beach vs. desert1\n",
    "print 'Desert2 vs. Desert1:\\t' + str(MycompHist(hist_desert2, hist_desert1)) # desert2 vs. desert1\n",
    "print 'Beach vs. Desert2:\\t' + str(MycompHist(hist_beach, hist_desert2)) # beach vs. desert2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Similar Image Search by Histogram Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the news**: Flickr and Pinterest rolled out update last week (March 7th, 2017) to allow users to search for images based on visual similarity. Flickr in their blog post says it uses deep neural networks to analyze the photos as they are uploaded to the server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, using the histogram as a feature for our image, let's see how well it works when searching for similar images in a large dataset. You will need to import the lab5.py file, as done below, to use a function to load the Caltech-101 dataset. You can download from [here](http://www.vision.caltech.edu/Image_Datasets/Caltech101/) and unzipped this dataset to the same directory as your notebook. It should be in a folder called '101_ObjectCategories.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lab5\n",
    "\n",
    "dataset_dir = '101_ObjectCategories/'\n",
    "\n",
    "n_images, classes, image_names = lab5.load_image_dataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates histograms for all of the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hists = []\n",
    "\n",
    "n_bins = 100\n",
    "\n",
    "for name in image_names:\n",
    "    temp_image = misc.imread(name)\n",
    "    temp_hist = generateHist(temp_image, n_bins)\n",
    "    hists.append(temp_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the four test query images that we will use to search for similar images. Notice the diversity in their histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare a test histogram to the database\n",
    "test_ind = [450, 2423, 5211, 8134]\n",
    "n_bins = 100\n",
    "\n",
    "for i in test_ind:\n",
    "    test = misc.imread(image_names[i])\n",
    "    plt.figure()\n",
    "    plt.imshow(test)\n",
    "\n",
    "    test_hist = generateHist(test, n_bins)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(test_hist[0,:], color='red')\n",
    "    plt.plot(test_hist[1,:], color='green')\n",
    "    plt.plot(test_hist[2,:], color='blue')\n",
    "    plt.title('Histogram for Image ' + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Finding similar images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below iterates through the above test query images and searches for their 10 most similar images (K is the number of similar images in the code). Fill in the missing elements to generate the histograms and compare them using your own histogram implementation from Exercise 1. The number of bins for the histogram should be n_bins = 100.\n",
    "\n",
    "The images will be stored in a directory called \"closest\" in the same location as your notebook. They will have suffixes denoting which of the four images they match and their order in terms of similarity. (Note that the first image should be the query image itself, since it is still in the dataset and is more similar to itself than any other image). Submit this folder with your notebook for the lab.\n",
    "\n",
    "Do the results look reasonable? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Documentation on [argsort](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "closest_dir = 'closest/'\n",
    "if not os.path.exists(closest_dir):\n",
    "    os.makedirs(closest_dir)\n",
    "\n",
    "n_bins = 100\n",
    "K = 10\n",
    "    \n",
    "for ind in test_ind:\n",
    "    print 'Finding closest ' + str(K) + ' images to ' + image_names[ind]\n",
    "    test = misc.imread(image_names[ind])\n",
    "    \n",
    "    test_hist = [] # edit this line to make things right\n",
    "\n",
    "    D = np.zeros(n_images)\n",
    "    for i in range(n_images):\n",
    "        D[i] =  0 # edit this line to make things right \n",
    "    idx = np.argsort(D)\n",
    "    \n",
    "    closest = []\n",
    "    for i in range(K):\n",
    "        closest.append(image_names[idx[i]])\n",
    "        img = misc.imread(closest[i])\n",
    "        misc.imsave(closest_dir + 'closest_' + str(ind) + '_' + str(i) + '.jpg', img)\n",
    "        print '\\tSaving closest image: ' + image_names[idx[i]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer (within 2 lines): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Histogram of Oriented Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn the details of the Histogram of Oriented Gradients (HOG) feature descriptor. You are strongly encouraged to read this blog first: http://www.learnopencv.com/histogram-of-oriented-gradients/. It contains the details of the implementations. First, let's see the HOG feature by OpenCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "hog = cv2.HOGDescriptor()\n",
    "im = cv2.imread('Duncan.jpg')\n",
    "\n",
    "h = hog.compute(im)  # HoG feature by OpenCV\n",
    "plt.plot(h)\n",
    "\n",
    "im = np.fliplr(im.reshape(-1,3)).reshape(im.shape)  #convert BGR to RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we are going to calculate the HOG feature of the above image patch by ourself. You are allowed to use OpenCV functions but are not allowed to call HOG descriptor directly. \n",
    "\n",
    "**Step 1**: Calculate the Gradient Images. After we obtained the gradients the RGB channels, we just pick the channel with the strongest gradient magnitude. After this step, you should get the magnitude and direction of the gradient for each pixel, both of which are a 128\\*64 matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = np.float32(im)/255.0\n",
    "gx = cv2.Sobel(im, cv2.CV_32F, 1, 0, ksize=1)\n",
    "gy = cv2.Sobel(im, cv2.CV_32F, 0, 1, ksize=1)\n",
    "\n",
    "#plot the X and Y gradients.\n",
    "plt.figure()\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(im)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(np.abs(gx))\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(np.abs(gy))\n",
    "\n",
    "# Python Calculate gradient magnitude and direction ( in degrees ) \n",
    "mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "Pick the channel with the strongest gradient magnitude for each pixel. Convert the degree from [0,360] to [0,180] by ignoring the sign of the gradient direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#your code here. \n",
    "\n",
    "\n",
    "#plot your result\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(mag, cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(angle, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "**Step 2**: Calculate Histogram of Gradients in 8×8 cells.\n",
    "\n",
    "In this step, the image is divided into 8×8 cells and a histogram of gradients is calculated for each 8×8 cells. We create 9 bins, i.e. [0,20,40,60,80,100,120,140,160]. You will need to add the magnitude to the corresponding bin based on the direction. You should use interpolation. For example, 30 degree should be divided into half in 20 and half in 40. After this step, we should get 8\\*16 cells, and each cell has a histogram of gradients with 9 bins.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell_size = 8\n",
    "nbins = 9\n",
    "\n",
    "nH= im.shape[0]/cell_size \n",
    "nW = im.shape[1]/cell_size\n",
    "\n",
    "raw_hog = np.zeros((nH,nW,nbins),dtype=np.float32)   # define the histogram of gradients \n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot this cell for the TA to check correctness.         \n",
    "plt.plot(raw_hog[5,5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "**Step 3**: Block Normalization\n",
    "\n",
    "In this step, we will need to normlize the HOG features in every 16\\*16 blocks, which is every 4 cells. In each block there will be a 36\\*1 feature vector, which is normalized by its L2 norm. The window is then moved by 1 cell. So finally, we will have the 7\\*15 blocks with normalized features. We reshape them into one vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hog = np.zeros((nH-1,nW-1,nbins*4),dtype=np.float32)\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "hog = np.reshape(hog,[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare your implementation and the OpenCV implementation. They may not be the same. Think about why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hog)\n",
    "plt.figure()\n",
    "plt.plot(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6:  \n",
    "\n",
    "Extra credit (up to 30 points). Implement a pedestain detector based on your HOG features. You can download the INRIA Person Dataset for training and testing http://pascal.inrialpes.fr/data/human/. You can train a SVM to classify the neg and pos examples based on their HOG features. Print your test accuracy. And plot 3 images of true detect, 3 images of false detect, and 3 images of miss detect. You can use the OpenCV SVM package directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
